.. _howto-custom-test-rules:

*************************
Writing Custom Test Rules
*************************

For maximum flexibility in testing, you can write custom test rules using the
``rule`` stanza with the ``runtest`` alias. This approach gives you full
control over how tests are executed and validated.

Overview
========

The ``dune runtest`` command simply builds the ``runtest`` alias. By adding
actions to this alias using the ``rule`` stanza, you can define custom tests
that do anything a Dune rule can do.

This is useful for:

- Custom test frameworks not covered by other approaches
- Performance benchmarks
- Build-time validation
- Code generation tests
- Integration with external test tools

Basic Pattern
=============

The fundamental pattern is a rule with the ``runtest`` alias:

.. code:: dune

   (rule
    (alias runtest)
    (action <some-action>))

When you run ``dune runtest``, Dune will execute ``<some-action>``.

Running an Executable
=====================

The simplest custom test runs an executable:

.. code:: dune

   (executable
    (name my_tests))

   (rule
    (alias runtest)
    (action (run ./my_tests.exe)))

This is similar to using the ``test`` stanza, but gives you more control over
the action.

With Arguments
--------------

Pass arguments to your test executable:

.. code:: dune

   (rule
    (alias runtest)
    (action (run ./my_tests.exe --verbose --color=always)))

With Dependencies
-----------------

Declare dependencies explicitly:

.. code:: dune

   (rule
    (alias runtest)
    (deps my_tests.exe test_data.json config.toml)
    (action (run ./my_tests.exe)))

Diff Testing
============

A common pattern is to run a program and diff its output against expected
results.

Basic Diff Test
---------------

.. code:: dune

   (rule
    (alias runtest)
    (deps ./program.exe)
    (action
     (progn
      (with-stdout-to output.txt (run ./program.exe))
      (diff expected.txt output.txt))))

This:

1. Runs ``program.exe``
2. Captures its output to ``output.txt``
3. Compares ``output.txt`` to ``expected.txt``
4. Shows a diff if they don't match

If the output changes, use ``dune promote`` to copy ``output.txt`` to
``expected.txt`` in the source tree.

Diff Action Details
-------------------

The ``diff`` action takes two arguments:

.. code:: dune

   (diff <expected-file> <generated-file>)

- ``<expected-file>`` - The file in the source tree with expected content
- ``<generated-file>`` - The file generated by the build

If they differ, Dune will show a diff and ``dune promote`` will copy the
generated file to the expected file.

See :doc:`/concepts/promotion` for more on the promotion mechanism.

Multi-Step Testing
===================

Use ``progn`` to chain multiple actions:

.. code:: dune

   (rule
    (alias runtest)
    (deps program.exe input.txt)
    (action
     (progn
      (run ./program.exe preprocess input.txt preprocessed.txt)
      (run ./program.exe validate preprocessed.txt)
      (diff expected_output.txt preprocessed.txt))))

This runs multiple commands in sequence, stopping if any fails.

Conditional Tests
=================

Use ``enabled_if`` to run tests conditionally:

.. code:: dune

   (rule
    (alias runtest)
    (enabled_if (= %{system} linux))
    (action (run ./linux_specific_test.exe)))

Platform-Specific Tests
-----------------------

.. code:: dune

   (rule
    (alias runtest)
    (enabled_if (<> %{system} windows))
    (action (run ./unix_test.exe)))

Feature-Based Tests
-------------------

.. code:: dune

   (rule
    (alias runtest)
    (enabled_if %{lib-available:async})
    (deps test_async.exe)
    (action (run ./test_async.exe)))

Custom Aliases
==============

Create custom test aliases for different test types:

.. code:: dune

   (rule
    (alias runtest-unit)
    (action (run ./unit_tests.exe)))

   (rule
    (alias runtest-integration)
    (deps server.exe client.exe)
    (action (run ./integration_tests.exe)))

   (rule
    (alias runtest-benchmarks)
    (action (run ./benchmarks.exe --quick)))

Run specific test categories:

.. code:: console

   $ dune build @runtest-unit
   $ dune build @runtest-integration

The standard ``runtest`` alias can depend on these custom aliases:

.. code:: dune

   (alias
    (name runtest)
    (deps
     (alias runtest-unit)
     (alias runtest-integration)))

Testing Code Generation
========================

Test that generated code matches expectations:

.. code:: dune

   (rule
    (targets generated_code.ml)
    (deps generator.exe schema.json)
    (action (with-stdout-to %{targets} (run ./generator.exe %{deps}))))

   (rule
    (alias runtest)
    (action (diff expected_code.ml generated_code.ml)))

If the generator changes, ``dune promote`` updates the expected output.

Running External Tools
======================

Integrate external test tools:

Format Checking
---------------

.. code:: dune

   (rule
    (alias runtest)
    (deps (source_tree src))
    (action (run ocamlformat --check src/*.ml)))

Linting
-------

.. code:: dune

   (rule
    (alias runtest)
    (deps (source_tree src))
    (action (run pylint src/*.py)))

Custom Validators
-----------------

.. code:: dune

   (rule
    (alias runtest)
    (deps api_spec.json)
    (action (run openapi-validator api_spec.json)))

Performance Benchmarks
======================

Run benchmarks as part of testing:

Quick Benchmarks
----------------

.. code:: dune

   (rule
    (alias runtest-perf)
    (deps benchmark.exe)
    (action (run ./benchmark.exe --quick)))

Diff Benchmark Results
----------------------

.. code:: dune

   (rule
    (alias runtest-perf)
    (deps benchmark.exe)
    (action
     (progn
      (with-stdout-to results.txt (run ./benchmark.exe))
      (diff expected_results.txt results.txt))))

This lets you track performance regressions by comparing benchmark output.

Complex Test Scenarios
=======================

Multi-Program Integration
--------------------------

Test interaction between multiple programs:

.. code:: dune

   (rule
    (alias runtest)
    (deps server.exe client.exe test_data.json)
    (action
     (progn
      (run ./server.exe start --background)
      (run ./client.exe connect localhost:8080)
      (run ./client.exe send test_data.json)
      (run ./server.exe stop))))

Note: This example is simplified. Real background process management requires
more sophisticated handling.

File Transformation Tests
--------------------------

Test that a transformation produces expected output:

.. code:: dune

   (rule
    (alias runtest)
    (deps transformer.exe input.txt)
    (action
     (progn
      (with-stdout-to transformed.txt
       (run ./transformer.exe input.txt))
      (diff expected_transformed.txt transformed.txt))))

Best Practices
==============

Be Explicit About Dependencies
-------------------------------

Always declare dependencies in the ``deps`` field. Don't rely on files being
available implicitly:

.. code:: dune

   ; Good
   (rule
    (alias runtest)
    (deps test.exe data.txt)
    (action (run ./test.exe)))

   ; Bad - data.txt dependency not declared
   (rule
    (alias runtest)
    (action (run ./test.exe)))  ; Assumes data.txt exists

Use diff for Output Validation
-------------------------------

Prefer ``diff`` over manual assertions when testing output:

.. code:: dune

   ; Good - shows exact diff
   (rule
    (alias runtest)
    (action
     (progn
      (with-stdout-to out.txt (run ./prog.exe))
      (diff expected.txt out.txt))))

   ; Less ideal - just pass/fail
   (rule
    (alias runtest)
    (action (run ./test_program_output.exe)))

Keep Tests Fast
---------------

Slow tests discourage running them frequently. Consider:

- Using ``(alias runtest-slow)`` for expensive tests
- Making quick tests the default
- Caching results when possible

Organize by Test Type
----------------------

Use custom aliases to organize different test types:

.. code:: dune

   (alias
    (name runtest)
    (deps
     (alias runtest-fast)))

   (alias
    (name runtest-all)
    (deps
     (alias runtest-fast)
     (alias runtest-slow)
     (alias runtest-integration)))

When to Use Custom Rules
=========================

Choose custom test rules when:

- Other test approaches don't fit your needs
- You need precise control over test execution
- You're integrating external tools
- You're testing build artifacts, not just code behavior
- You need complex multi-step test procedures

For simpler cases, prefer:

- :doc:`/howto/inline-tests` for unit tests
- :doc:`/howto/expect-tests` for output testing
- :doc:`/howto/tests-stanza` for executable tests
- :doc:`/howto/cram-tests` for CLI testing

See Also
========

- :doc:`/reference/actions/index` - Complete action reference
- :doc:`/concepts/promotion` - Understanding diff and promote
- :doc:`/reference/aliases/runtest` - The runtest alias
- :doc:`/concepts/dependency-spec` - Specifying dependencies
- :doc:`/explanation/testing-overview` - Overview of all test types
